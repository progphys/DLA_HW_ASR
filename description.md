# Домашнее задание 2 (ASR)

> [!IMPORTANT]
> Ваш код **должен** быть основан на предоставленном [Project Template](https://github.com/Blinorot/pytorch_project_template). Разрешается выбрать любую ветку как начальную точку (может быть, ветка `main` покажется проще, чем `ASR`). Однако мы настоятельно рекомендуем ветку `ASR` для этой домашней работы.

Чтобы ознакомиться с шаблоном, рекомендуем посмотреть Tutorials из `README` и/или следующие семинары "Creating convenient DL pipelines and clean code" ([Часть I](https://github.com/markovka17/dla/tree/2025/week01) and [Часть II](https://github.com/markovka17/dla/tree/2025/week03)). В качестве начальной точки вы можете реализовать простую задачу Image Classification, используя ветку `main`, а затем сравнить своё решение с шаблонной версией.

### Условие задания

Реализуйте и обучите автоматическую систему распознавания речи (Automatic Speech Recognition, ASR): вариант на CTC или LAS.

> [!IMPORTANT]
> Вы **не можете** использовать любые существующие реализации доступные в интернете. Это включает в себя существующие GitHub/GitLab репозитории, реализации на HuggingFace или в библиотеках `espnet, nemo, speechbrain, torchaudio` и тому подобное.

---

### Обязательные требования

Мы **не принимаем** домашнюю работу, если одно из следующих требований не выполнено:

- Код должен быть размещён в публичном GitHub/GitLab‑репозитории и основываться на предоставленном шаблоне. (До дедлайна используйте приватный репозиторий. Сделайте его публичным после дедлайна.)
- Все необходимые пакеты должны быть перечислены в `./requirements.txt` или установлены в `dockerfile`.
- Все необходимые ресурсы (веса моделей, LMs) должны автоматически скачиваться с помощью скрипта. Укажите этот скрипт (или строки кода) в `README.md`.
- Вы должны реализовать все функции в `inference.py` (для инференса и подсчета метрик), чтобы мы могли проверить ваше задание.
- Скрипты `inference.py` и `train.py` должны запускаться без ошибок после выполнения всех команд из вашего раздела установки.
- Необходимо создать demo‑notebook (см. раздел [Демо](#демо)).
- Вы должны предоставить **логи обучения финальной модели** с самого начала обучения. Настоятельно рекомендуем использовать W&B (или CometML) Reports.
- Приложите краткий отчёт, включающий:
  - Как воспроизвести вашу модель (пример: обучаем 50 эпох с `train_1.yaml` и 50 эпох с `train_2.yaml`)
  - Логи обучения: как быстро обучалась сеть
  - Как вы обучали финальную модель
  - Что пробовали, какие эксперименты были сделаны, их анализ
  - Что сработало / не сработало
  - Основные сложности, с которыми вы столкнулись
  - Анализ графиков и экспериментов
  - Также приложите список реализованных бонусных заданий.

> [!NOTE]
> Если ваша версия Comet ML не поддерживает audio panels, вы можете использовать Python Panel, код предоставлен ниже (проверьте, что нормализация графика работает корректно для вашего аудио)

<details>

<summary>Audio Panel Code (as Python Comet ML Panel)</summary>

```python
# Comet Python Panels BETA, full documentation available at:
# https://www.comet.com/docs/v2/guides/comet-ui/experiment-management/visualizations/python-panel/

# By Petr Grinberg @ https://github.com/markovka17/dla 2024

from comet_ml import API, ui
from scipy.io.wavfile import read
import matplotlib.pyplot as plt
import os
import numpy as np
import streamlit as st


# Get available metrics
api = API()
metrics = api.get_panel_metrics_names()

exps = api.get_panel_experiments()
all_possible_steps = []
for exp in exps:
    assets = exp.get_asset_list()
    for asset in assets:
        if asset["type"] == "audio":
            step = asset["step"]
            all_possible_steps.append(step)
all_possible_steps = sorted(set(all_possible_steps))

step_option = st.selectbox(
    "Choose a step",
    all_possible_steps,
)

for exp in exps:
    exp_name = exp.name
    assets = exp.get_asset_list()
    for asset in assets:
        if asset["type"] == "audio":
            curl_command = asset["curlDownload"]
            filename = asset["fileName"]
            step = asset["step"]
            if step != step_option:
                continue
            os.system(curl_command)
            sr, wav_array = read(curl_command.split()[-1])
            print(f"Exp: {exp_name}, Step: {step}, Name: {filename}")
            wav_dtype = wav_array.dtype
            max_amplitude = np.iinfo(np.int16).max
            wav_array = wav_array / max_amplitude
            # Visualize the data
            figure, ax = plt.subplots()
            wav_time = np.arange(wav_array.shape[0]) / sr
            ax.plot(wav_time, wav_array)
            ax.set_xlabel("Time (s)")
            ax.grid()
            st.pyplot(figure)
            st.audio(wav_array, format="audio/wav", sample_rate=sr, loop=False)
```

</details>

---

### Оценивание

Оценка будет зависеть от качества модели и качества кода/отчёта:

```
grade = quality_score - implementation_penalties - report_penalties
```

---

### Штрафы

Ниже указано, за что снимаются баллы:

- (до `-10`) Логирование. Логи W&B/CometML должны включать:

  - Текстовые отчёты со случайными примерами (`target`, `prediction`, CER, WER)
  - Изображения train/valid спектрограм \ аудио
  - Норму градиента (Gradient norm)
  - Шаг обучения (Learning rate)
  - Ошибку на каждом шаге (Loss)

- (до `-10`) Вы должны реализовать минимум 4 релевантные ASR-аугментации. Чтобы не получить штраф, вы должны (**Ваша финальная модель может быть обучена без аугментаций, но все равно реализовать аугментации и сделать указанное ниже необходимо**):
  - Показать Аудио/спектрограммы до\после аугментаций
  - Убедиться, что аугментации действительно работают как ожидается
  - Убедиться, что вы не переборщили с ними и модель может обучаться
- (до `-20`) Вы должны реализовать простой hand-crafted beam search (CTC/LAS) сами. Использовать библиотеки для этой реализации запрещено. Для того, чтобы не получить штраф, нужно показать, что beam search работает не хуже greedy (argmax) подхода на ваших данных (то есть необходимо показать две метрики).

> [!NOTE]
> Одна из целей этого домашнего задания -- научить вас правильному написанию кода. Поэтому, мы может смотреть на историю ваших комитов в Git, `README`, `requirements.txt`, и т.д.. Оформляйте ваш репозиторий так, как будто вы показываете его вашим будущим работодателям. `README` должно быть подробным и описывать ваш репозиторий. В `requirements` не должно быть лишних пакетов, которые на деле не используютсяю. История комитов должна пояснять, что каждый комит делает. Используйте `pre-commit` для форматирования кода (в шаблоне есть поддержка `black` и `isort`).

---

### Оценка качества моделей

Таблицы порогов для CTC:

| Score | Dataset                     | CER | WER | Description                                  |
| ----- | --------------------------- | --- | --- | -------------------------------------------- |
| 10    | --                          | --  | --  | Вы попытались                                |
| 20    | LibriSpeech: test-clean     | 50  | --  | Хотя бы что-то                               |
| 30    | LibriSpeech: test-clean     | 30  | --  | В целом можно угадать фразу                  |
| 40    | LibriSpeech: test-clean     | 20  | --  | Какие-то слова правильные                    |
| 50    | LibriSpeech: test-clean     | --  | 40  | Больше половины слов похожи на правду        |
| 60    | LibriSpeech: test-clean     | --  | 30  | Читаемо                                      |
| 70    | LibriSpeech: test-clean     | --  | 20  | Ошибки лишь время от времени                 |
| 80    | LibriSpeech: **test-other** | --  | 30  | Модель может работать как-то с шумными аудио |
| 85    | LibriSpeech: **test-other** | --  | 25  | Еще лучше работа с шумными аудио             |
| 90    | LibriSpeech: **test-other** | --  | 20  | Может применяться для практических задач     |
| 95    | LibriSpeech: **test-other** | --  | 15  | Близко к качеству человека                   |
| 100   | LibriSpeech: **test-other** | --  | 10  | Технически лучше чем человек!                |

Если ваша финальная модель это LAS, то оценивание идет по таблице ниже, а также вам дается **+1 балл за реализацию**

| Score | Dataset                     | CER (LAS) | WER (LAS) |
| ----- | --------------------------- | --------- | --------- |
| 10    | --                          | --        | --        |
| 20    | LibriSpeech: test-clean     | 50        | --        |
| 30    | LibriSpeech: test-clean     | 30        | --        |
| 40    | LibriSpeech: test-clean     | 10        | --        |
| 50    | LibriSpeech: test-clean     | --        | 30        |
| 60    | LibriSpeech: test-clean     | --        | 20        |
| 70    | LibriSpeech: test-clean     | --        | 15        |
| 80    | LibriSpeech: **test-other** | --        | 25        |
| 85    | LibriSpeech: **test-other** | --        | 20        |
| 90    | LibriSpeech: **test-other** | --        | 15        |
| 95    | LibriSpeech: **test-other** | --        | 12        |
| 100   | LibriSpeech: **test-other** | --        | 8         |

---

### Тестирование

Вы **обязаны** добавить скрипт `inference.py` и класс `CustomDirDataset` в `src/datasets/` + конфиг в `src/configs/`.

`CustomDirDataset` должен поддерживать следующий формат данных:

```bash
NameOfTheDirectoryWithUtterances
├── audio
│   ├── UtteranceID1.wav # may be flac or mp3
│   ├── UtteranceID2.wav
│   .
│   .
│   .
│   └── UtteranceIDn.wav
└── transcriptions # ground truth, may not exist
    ├── UtteranceID1.txt
    ├── UtteranceID2.txt
    .
    .
    .
    └── UtteranceIDn.txt
```

`CustomDirDataset` должен принимать путь через `Hydra`.

`inference.py` должен прогонять модель на заданном датасете (кастомном и любой другой из `src/datasets`) и сохранять предсказания в папку, причём имена файлов должны совпадать с ID аудио-файла, чтобы их можно быть сопоставить.

**Добавьте отдельный скрипт `calc_metrics.py`** для подсчета WER/CER на передающихся путях до настоящих транскрипций и ваших предсказаний.

В `README` должно быть сказано как запустить инференс и подсчет метрик.

### Демо

Для того, чтобы другие могли с легкостью пользоваться вашим решением и понимать как работает ваш код, хорошей практикой является создание демо (дополнительно к хорошим инструкциям в README), например, в виде IPython-ноутбука. **Вы должны приложить такой демо-ноутбук в ваш Git**.

Демо-ноутбук должен делать следующие:

1. Клонирует ваш репозиторий и выполняет шаги установки из `README`.
2. Скачивает все необходимые веса (модели, LM).
3. Показывает, как запускать `inference.py` и `calc_metrics.py`.
4. Позволяет пользователю **вставить GDrive ссылку** на свой датасет (формат из раздела [Тестирование](#тестирование)) и получить WER/CER.
5. Все это сопровождается комментариями и пояснениями.

> [!IMPORTANT]
> Вы **обязаны** проверить ваш инференс и убедиться, что он работает без проблем, иначе вы получите $0$ за домашнее задание. Это связано с тем, что ваш код должен быть рабочим и проверка его работоспособности -- задача разработчика, а не пользователей репозитория. Это также поможет вам исправить какие-то ошибки в вашем коде. Мы будем использовать ваш демо для проверки вашего решения. **Пользователь вашего демо должен просто запустить ячейки и вставить ссылку на Google диск, вы не должны ожидать от пользователя чего-то большего.**

> [!IMPORTANT]
> Ваш демо ноутбук должен запускаться в **Google Colab** без проблем.

---

### Бонусы

- **(+5)** Внешняя языковая модель (Language Model) для инференса. Вы можете реализовать любой LM-fusion. **Реализация этого бонуса позволит вам значительно улучшить WER/CER и соответственно также получить лучше оценку за качество**
- **(+10)** Использование BPE вместо символов. Можно применять SentencePiece / HuggingFace / YouTokenToMe.

> [!NOTE]
> Вы можете использовать внешие готовые библиотеки и веса для LM. Но вам все еще нужно самостоятельно написать и проверить рукописный beam_search (см. [Штрафы](#штрафы)). Аналогично обычному beam search, бонус засчитывается только если вы докажете что ваш LM работает лучше чем greedy-decoding.

> [!NOTE]
> Тоже самое касается BPE. Бонус засчитывается только если вы покажете две модели с\без BPE, где BPE-версия будет иметь не хуже качество, чем обычная модель.

---

### Дополнительные штрафы\бонусы

За очень красивый\эффективный код, красивый отчет, нестандартные подходы, вам могут быть начислены дополнительные баллы. Однако, вам также могут снять баллы если вы напишете очень плохой отчет или плохой код.

> [!IMPORTANT]
> Целью вашей работы является выработка ваших навыков инженера, а не ваших prompt-скиллов. Поэтому использование любых LLM строго запрещено. Если вы хотите чему-то научиться на курсе, то откажитесь от использования этих моделей.

---

### Рекомендации

Вы можете использовать любую архитектуру и даже придумать свою, но базовые рекомендации следующие:

- [DeepSpeech2](http://proceedings.mlr.press/v48/amodei16.pdf)
- [Conformer](https://arxiv.org/abs/2005.08100)

> [!NOTE]
> Если статья делает одну функцию потерь, никто не запрещает попробовать и другую. Например можно сделать Conformer-CTC, а можно Conformer-LAS.

Воспроизвести статью -- сложная задача, однако следующая последовательность шагов упростит разработку и отладку вашего кода:

1. Если ваш пайплайн работает, то он должен суметь получить идеальную метрику на одном батче (из одного-нескольких элементов). То есть один и тот же батч с одними и теми же элементами подается как на train, так и не test. Это называется One-Batch Test.
2. Обучайтесь только на LibriSpeech пока не сможете достичь 30 WER на clean данных.
3. Дообучитесь на смеси LibriSpeech + Common Voice (для улучшения качества на test-other) или добавьте аугментации.

> [!IMPORTANT]
> Количество ваших ГПУ-часов ограничено, используйте их эффективно. Мы предлагаем использовать следующие бесплатные ресурсы Google Colab (8h/day) и Kaggle (30h/week). Используйте Colab для дебага и Kaggle для длительного обучения. Не забывайте скачивать веса, чтобы можно было продолжить обучение с места остановки после лимита времени сессии. Все домашнее задание можно сделать в Kaggle. Чтобы не скачивать лишний раз, используйте [Kaggle датасет](https://www.kaggle.com/datasets/a24998667/librispeech).

Дополнительные советы о том как начать делать задание (все ниже можно сделать локально на CPU без использования ГПУ-часов):

0. Изучите ветки `main` и `example/image_classification`. Они содержат полностью рабочие пайплайны, которые помогут вам понять как работает шаблон и как в нем писать. В частности, вы можете сначала попробовать написать свою версию Image Classification, а потом сравнить с примером.
1. Заполните все TODO в шаблоне в ветке ASR.
2. Настройте logging, Dataset/Dataloader, Model init, Trainer init. Все это можно сделать без вычислительных затрат
3. Проверьте входы/выходы модели, их размерности и что они совпадают с ожидаемыми.
4. Прогоните One-Batch epoch (отключив при этом подсчет градиентов и шаг оптимизации), чтобы убедиться, что логирование и данные работают.
5. Запустите простую модель, например MLP, на One-Batch test для проверки пайплайна (CER/WER должны оказаться около 0).
6. Начните реализацию модели по вашему выбору, также сделайте One-Batch test
7. Пробуйте запускать в Colab\Kaggle

Полезные ссылки:

- [Mozilla Common Voice (en)](https://commonvoice.mozilla.org/ru)
- [LibriSpeech](https://www.openslr.org/12)

Вы также можете использовать [HuggingFace dataset library](https://github.com/huggingface/datasets). Например:

```python
from datasets import load_dataset
dataset = load_dataset("librispeech_asr", split='train-clean-360')
```

Для загрузки весов вашей модели (для сдачи задания) вы можете сохранить эти веса на Google диск или в HuggingFace Models.
